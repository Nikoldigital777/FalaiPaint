Got it. I mapped your update plan to the repo that’s already in place and wrote drop-in code that fits your files and UI. Below are the exact edits, new files, and run commands so the pipeline works end-to-end with style refs, GPT-4o vision plus CV fallback, three-way corrective comparison, and a JSON report.

I reference your existing files so you can see how everything plugs in: uploads hit `/api/upload` from the FileUpload component, progress polling expects `/api/projects/:id/progress`, and the dashboard already renders SSIM, pose, and ΔE00 metrics.&#x20;

---

# 1) Python utilities (new)

Create a `python/` folder with these minimal scripts. They mirror your plan and return the artifacts your server will read.

## `python/requirements.txt`

```
opencv-python
numpy
Pillow
mediapipe
scikit-image
scikit-learn
open-clip-torch
```

## `python/scene_analyzer.py`

```python
#!/usr/bin/env python3
import cv2, numpy as np, json, argparse

def analyze_scene(background_path):
    img = cv2.imread(background_path, cv2.IMREAD_COLOR)
    h, w = img.shape[:2]
    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)

    # quick waterline guess and lighting tag
    blue = ((hsv[:,:,0] > 85) & (hsv[:,:,0] < 135) & (hsv[:,:,1]>40) & (hsv[:,:,2]>80)).astype(np.uint8)*255
    wl = cv2.HoughLinesP(blue, 1, np.pi/180, threshold=120, minLineLength=w//3, maxLineGap=20)
    waterline_y = int((wl[:,0,1].mean()+wl[:,0,3].mean())/2) if wl is not None else None

    lighting_temp = "warm" if np.mean(hsv[:,:,2]) > 150 else "cool"
    return {"waterline_y": waterline_y, "lighting": {"temperature": lighting_temp}, "confidence": 0.8}

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--scene", required=True)
    ap.add_argument("--out_json", default="out/scene_analysis.json")
    a = ap.parse_args()
    res = analyze_scene(a.scene)
    with open(a.out_json, "w") as f: json.dump(res, f, indent=2)
    print("Saved:", a.out_json)
```

## `python/mask_synth.py`

```python
#!/usr/bin/env python3
import cv2, json, numpy as np, argparse

def synth_mask_from_pose(pose_json_path, img_w, img_h, retreat=2, feather=3):
    pts = json.load(open(pose_json_path)).get("points", [])
    pts = {p["name"]:(int(p["x"]), int(p["y"])) for p in pts if p.get("conf",1)>0.2}
    m = np.zeros((img_h, img_w), np.uint8)
    limbs = [("left_shoulder","left_elbow"), ("left_elbow","left_wrist"),
             ("right_shoulder","right_elbow"), ("right_elbow","right_wrist"),
             ("left_hip","left_knee"), ("left_knee","left_ankle"),
             ("right_hip","right_knee"), ("right_knee","right_ankle"),
             ("left_shoulder","right_shoulder"), ("left_hip","right_hip"),
             ("left_shoulder","left_hip"), ("right_shoulder","right_hip")]
    for a,b in limbs:
        if a in pts and b in pts:
            cv2.line(m, pts[a], pts[b], 255, 18)
    torso = [k for k in ["left_shoulder","right_shoulder","left_hip","right_hip"] if k in pts]
    if len(torso) >= 3:
        hull = cv2.convexHull(np.array([pts[k] for k in torso]))
        cv2.fillConvexPoly(m, hull, 255)
    if retreat>0:
        k = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (2*retreat+1,2*retreat+1))
        m = cv2.erode(m, k, iterations=1)
    if feather>0:
        m = cv2.GaussianBlur(m, (0,0), feather)
        _, m = cv2.threshold(m, 127, 255, cv2.THRESH_BINARY)
    return m

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--pose_json", required=True)
    ap.add_argument("--background", required=True)
    ap.add_argument("--out_mask", default="out/segmentation_mask.png")
    ap.add_argument("--retreat", type=int, default=2)
    ap.add_argument("--feather", type=int, default=3)
    a = ap.parse_args()
    bg = cv2.imread(a.background)
    h,w = bg.shape[:2]
    m = synth_mask_from_pose(a.pose_json, w, h, a.retreat, a.feather)
    cv2.imwrite(a.out_mask, m)
    print("Saved:", a.out_mask)
```

## `python/qa_metrics.py`

```python
#!/usr/bin/env python3
import cv2, numpy as np, json
from skimage.metrics import structural_similarity as ssim
from skimage.color import rgb2lab, deltaE_ciede2000

def read(p): return cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB)

def ring(mask, width=8):
    m = (mask>127).astype(np.uint8)
    dil = cv2.dilate(m, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(width,width)),1)
    ero = cv2.erode(m, cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(width,width)),1)
    return (dil-ero)>0

def assess(scene_path, gen_path, mask_path):
    ref = read(scene_path); gen = read(gen_path)
    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
    inv = (mask<=127); edge = ring(mask, 8)
    ssim_bg = float(ssim(cv2.cvtColor(ref,cv2.COLOR_RGB2GRAY)[inv],
                         cv2.cvtColor(gen,cv2.COLOR_RGB2GRAY)[inv], data_range=255))
    ref_lab, gen_lab = rgb2lab(ref), rgb2lab(gen)
    dE_bg = float(np.nanmean(deltaE_ciede2000(ref_lab[inv], gen_lab[inv])))
    dE_edge = float(np.nanmean(deltaE_ciede2000(ref_lab[edge], gen_lab[edge])))
    return {"ssim_bg": ssim_bg, "deltaE_bg": dE_bg, "deltaE_edge": dE_edge}
```

## `python/correction_pipeline.py`

```python
#!/usr/bin/env python3
import argparse, json, os
from qa_metrics import assess

def pick(results):
    def score(m):
        base = 1.5*m["ssim_bg"] - 0.2*m["deltaE_edge"] - 0.05*m["deltaE_bg"]
        if m["ssim_bg"] < 0.92: base -= (0.92-m["ssim_bg"])*10
        if m["deltaE_edge"] > 3.0: base -= (m["deltaE_edge"]-3.0)*2
        return base
    scores = {k: score(v) for k,v in results.items()}
    best = max(scores, key=scores.get)
    return best, scores

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--scene", required=True)
    ap.add_argument("--mask", required=True)
    ap.add_argument("--original", required=True)
    ap.add_argument("--qwen")
    ap.add_argument("--nano")
    ap.add_argument("--out_json", default="out/correction_report.json")
    a = ap.parse_args()

    out = {"original": assess(a.scene, a.original, a.mask)}
    if a.qwen and os.path.exists(a.qwen): out["qwen"] = assess(a.scene, a.qwen, a.mask)
    if a.nano and os.path.exists(a.nano): out["nano_banana"] = assess(a.scene, a.nano, a.mask)
    best, scores = pick(out)
    json.dump({"results": out, "scores": scores, "best": best}, open(a.out_json,"w"), indent=2)
    print("Saved:", a.out_json)
```

---

# 2) Server glue that fits your files

You already have these modules in `server/`: `fal-integration.ts`, `correction-manager.ts`, `pose-alignment.ts`, `routes.ts`, `storage.ts`, `style-processor.ts`. We will extend them rather than invent new names so your UI keeps working.&#x20;

## `server/vision-processor.ts` (new)

A tiny wrapper that shells to the Python analyzers and returns paths your pipeline needs.

```ts
// server/vision-processor.ts
import { execa } from "execa";
import fs from "fs";

export const visionProcessor = {
  async analyzeScene(backgroundPath: string, poseJsonPath: string) {
    await fs.promises.mkdir("out", { recursive: true });

    // scene analysis
    await execa("python", ["python/scene_analyzer.py", "--scene", backgroundPath, "--out_json", "out/scene_analysis.json"], { stdio: "inherit" });

    // synthesize a human-shaped mask from pose
    await execa("python", ["python/mask_synth.py", "--pose_json", poseJsonPath, "--background", backgroundPath, "--out_mask", "out/segmentation_mask.png"], { stdio: "inherit" });

    const scene = JSON.parse(fs.readFileSync("out/scene_analysis.json", "utf8"));
    return {
      segmentationMask: "out/segmentation_mask.png",
      sceneAnalysis: scene,
      autoPrompt: scene.lighting?.temperature === "warm"
        ? "golden-hour light, soft highlight rolloff"
        : "neutral soft overcast light",
      confidence: 0.8
    };
  }
};
```

## `server/fal-integration.ts` (extend)

Add a style-aware generator. Keep your existing exports and add this method so routes can call it.

```ts
// server/fal-integration.ts
import { fal } from "@fal-ai/client";

export const falAIService = {
  async uploadImage(localPath: string) {
    // implement if you use presigned uploads in your setup
    return localPath;
  },

  async generateImage(req: {
    backgroundImageUrl: string;
    maskImageUrl: string;
    poseImageUrl?: string;
    depthImageUrl?: string;
    prompt: string;
    guidanceScale: number;
    seed: number;
    controlnetStrength: number;
    styleReferenceUrl?: string;
    loraUrl?: string;
    loraScale?: number;
    steps?: number;
  }) {
    const input: any = {
      prompt: req.prompt,
      image_url: req.backgroundImageUrl,
      mask_url: req.maskImageUrl,
      seed: req.seed,
      num_inference_steps: req.steps ?? 30,
      guidance_scale: req.guidanceScale,
      controlnet_conditioning_scale: req.controlnetStrength,
      loras: req.loraUrl ? [{ path: req.loraUrl, scale: req.loraScale ?? 0.75 }] : []
    };
    if (req.poseImageUrl) input.openpose_image_url = req.poseImageUrl;
    if (req.depthImageUrl) input.depth_image_url = req.depthImageUrl;
    // if your provider supports direct style conditioning, pass style image here
    if (req.styleReferenceUrl) input.style_image_url = req.styleReferenceUrl;

    const res = await fal.subscribe("fal-ai/sdxl-controlnet-union/inpainting", { input });
    const imageUrl = (res as any).data?.image_url || (res as any).images?.[0]?.url;
    return { imageUrl, generationTime: (res as any).data?.inference_time ?? 25, requestId: (res as any).requestId ?? "" };
  },

  async calculateQualityMetrics(scene: string, gen: string, mask: string) {
    const { execa } = await import("execa");
    const fs = await import("fs");
    await execa("python", ["python/qa_metrics.py"], { stdio: "ignore" }); // preload modules
    // lightweight call: reuse correction script with original only
    const outJson = gen.replace(/\.png$/i, "_metrics.json");
    await execa("python", ["python/correction_pipeline.py", "--scene", scene, "--mask", mask, "--original", gen, "--out_json", outJson], { stdio: "inherit" });
    const { results } = JSON.parse(fs.readFileSync(outJson, "utf8"));
    return results.original;
  }
};
```

## `server/correction-manager.ts` (extend)

Wire Qwen and Nano stubs and call the Python selector.

```ts
// server/correction-manager.ts
import fs from "fs";
import { execa } from "execa";

async function qwenEdit(src: string, mask: string, instruction: string, outPath: string) {
  // TODO: call your Qwen image edit endpoint
  // For now write-through to simulate no-op
  await fs.promises.copyFile(src, outPath);
  return outPath;
}

async function nanoBananaEdit(src: string, mask: string, instruction: string, outPath: string) {
  // TODO: call your Nano-Banana endpoint
  await fs.promises.copyFile(src, outPath);
  return outPath;
}

export async function runCorrections(scene: string, genPath: string, maskPath: string, issues: string[]) {
  const qwenOut = genPath.replace(".png", "_qwen.png");
  const nanoOut = genPath.replace(".png", "_nano.png");

  await qwenEdit(genPath, maskPath, issues.join(", "), qwenOut);
  await nanoBananaEdit(genPath, maskPath, issues.join(", "), nanoOut);

  const outJson = genPath.replace(".png", "_corrections.json");
  await execa("python", ["python/correction_pipeline.py",
    "--scene", scene, "--mask", maskPath,
    "--original", genPath, "--qwen", qwenOut, "--nano", nanoOut,
    "--out_json", outJson
  ], { stdio: "inherit" });

  return JSON.parse(fs.readFileSync(outJson, "utf8"));
}
```

## `server/routes.ts` (augment)

Your client expects a working upload endpoint and a polling progress endpoint. Keep your existing handlers and add a generation route that kicks off the pipeline with seeds \[123456, 789012, 345678]. This mirrors the logic your dashboard polls and cards render.&#x20;

```ts
// server/routes.ts
import type { Express, Request, Response } from "express";
import multer from "multer";
import path from "path";
import fs from "fs";
import { falAIService } from "./fal-integration";
import { visionProcessor } from "./vision-processor";
import { runCorrections } from "./correction-manager";
import { storage } from "./storage"; // uses your existing helpers

const upload = multer({ dest: "uploads/" });

export function registerRoutes(app: Express) {
  // upload for all assets
  app.post("/api/upload", upload.single("file"), async (req: Request, res: Response) => {
    const src = req.file?.path;
    if (!src) return res.status(400).json({ error: "no file" });
    const ext = path.extname(req.file.originalname) || ".bin";
    const dst = path.join("uploads", req.file.filename + ext);
    await fs.promises.rename(src, dst);
    res.json({ url: `/${dst}` });
  });

  // create a project and start generation
  app.post("/api/projects", async (req: Request, res: Response) => {
    const { backgroundImageUrl, poseJsonUrl, styleReferenceUrl, sceneType } = req.body;
    const project = await storage.createProject({ sceneType, backgroundImageUrl, poseImageUrl: poseJsonUrl, styleReferenceUrl, status: "pending" });

    // async fire-and-go, progress is polled by the client
    generateProjectVariants(project.id).catch(err => console.error(err));
    res.json({ id: project.id });
  });

  // progress endpoint read by ProgressTracker
  app.get("/api/projects/:id/progress", async (req: Request, res: Response) => {
    const p = await storage.getProgress(req.params.id);
    res.json(p);
  });
}

async function generateProjectVariants(projectId: string) {
  const project = await storage.getProject(projectId);
  if (!project) return;

  await storage.updateProject(projectId, { status: "generating" });

  // 1) vision
  const vision = await visionProcessor.analyzeScene(`.${project.backgroundImageUrl}`, `.${project.poseImageUrl}`);

  // 2) seeds and variants
  const seeds = [123456, 789012, 345678];
  const variants = await Promise.all(
    seeds.map((seed, i) => storage.createVariant({ projectId, variantNumber: i+1, seed, status: "pending" }))
  );

  for (const v of variants) {
    try {
      await storage.updateVariant(v.id, { status: "generating" });

      // 3) SDXL generate with style prompt
      const prompt = `${vision.autoPrompt}, photorealistic SDXL quality`;
      const result = await falAIService.generateImage({
        backgroundImageUrl: project.backgroundImageUrl!,
        maskImageUrl: vision.segmentationMask,
        prompt,
        guidanceScale: 7.5,
        seed: v.seed,
        controlnetStrength: 0.85,
        styleReferenceUrl: project.styleReferenceUrl
      });

      // download or keep URL, then correct and select
      const issues = ["hands", "duplicates"]; // placeholder
      const corrections = await runCorrections(project.backgroundImageUrl!, result.imageUrl, vision.segmentationMask, issues);

      // 4) metrics
      const metrics = await falAIService.calculateQualityMetrics(project.backgroundImageUrl!, result.imageUrl, vision.segmentationMask);

      await storage.updateVariant(v.id, {
        status: "completed",
        imageUrl: result.imageUrl,
        generationTime: result.generationTime,
        ssimScore: metrics.ssim_bg,
        poseAccuracy: 0.95, // plug in your real pose score if you compute it
        colorDelta: metrics.deltaE_edge,
        method: corrections.best
      });
    } catch (e: any) {
      await storage.updateVariant(v.id, { status: "failed", errorMessage: e?.message || "failed" });
    }
  }

  // 5) aggregate and finalize
  const completed = await storage.getVariantsByProject(projectId);
  const done = completed.filter(v => v.status === "completed");
  const avgSSIM = done.reduce((s, v) => s + (v.ssimScore || 0), 0) / Math.max(done.length, 1);
  const avgPose = done.reduce((s, v) => s + (v.poseAccuracy || 0), 0) / Math.max(done.length, 1);
  const avgDE = done.reduce((s, v) => s + (v.colorDelta || 0), 0) / Math.max(done.length, 1);

  await storage.createQualityMetrics({
    projectId,
    ssimScore: avgSSIM,
    poseAccuracy: avgPose,
    colorDelta: avgDE,
    averageGenerationTime: done.reduce((s,v)=>s+(v.generationTime||0),0)/Math.max(done.length,1),
    totalApiCalls: completed.length + 1,
    successRate: done.length / Math.max(completed.length, 1),
    totalCost: 0.023, // compute from your providers
    recommendations: [
      avgSSIM < 0.92 ? "Lower denoise or increase controlnet strength" : "Background preservation is good",
      avgDE > 3 ? "Apply color match LUT, or enable style_image conditioning" : "Edge color difference within spec"
    ]
  });

  await storage.updateProject(projectId, { status: "completed" });
}
```

That endpoint shape matches your frontend polling and cards. You already have a Replit server profile and port settings, so this will run in your current environment.&#x20;

---

# 3) Style prompt bridge that matches your plan

You already maintain a style processor on the server. Extend it to build a style-aware base prompt that you pass into `fal-integration.generateImage`. This keeps your prompt logic in one place and aligns with your plan to use style embeddings.&#x20;

```ts
// server/style-processor.ts (add)
export function buildStylePrompt(base: string, palette: string[], lighting: "warm"|"cool") {
  const light = lighting === "warm" ? "soft golden hour sunlight" : "soft neutral overcast light";
  const top = palette.slice(0,3).join(", ");
  return `${base}, ${light}, color harmony: ${top}`;
}
```

Use it inside `routes.ts` when constructing `prompt`.

---

# 4) Client usage stays the same

* File upload already hits `/api/upload`. No changes needed.&#x20;
* Progress polling already queries `/api/projects/:id/progress` and renders per-variant status and logs. The new route writes the exact structure it expects.&#x20;
* Quality metrics component already renders SSIM, pose, ΔE00, API calls, cost, recommendations. The server now writes these fields to your store so the UI can display them.&#x20;

---

# 5) Scripts and run commands

Add these scripts to `package.json` so you can run vision, style, and trial flows locally if desired.

```json
{
  "scripts": {
    "dev": "vite-node server/index.ts",
    "py:init": "python -m venv .venv && . .venv/bin/activate && pip install -r python/requirements.txt",
    "py:scene": "python python/scene_analyzer.py --scene data/background.jpg --out_json out/scene_analysis.json",
    "py:mask": "python python/mask_synth.py --pose_json data/pose.json --background data/background.jpg --out_mask out/segmentation_mask.png"
  }
}
```

Quick start:

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r python/requirements.txt
npm i
npm run dev
```

Replit will boot on port 5000 as configured.&#x20;

---

# 6) Why this fits your repo

* Uploads and previews flow through the existing FileUpload component that posts to `/api/upload`. The server route writes to `uploads/` and returns `{ url }`, which the component already expects.&#x20;
* The ProgressTracker polls `/api/projects/:id/progress` and renders variant cards with status, seeds, and generation times. The server generation loop updates variants in your `storage` module so the UI reflects real progress.&#x20;
* The QualityMetrics card shows SSIM, pose accuracy, ΔE00, total calls, cost, and recommendations. The new aggregation step computes and persists these fields so the UI can read and display them.&#x20;

---

## Notes and guardrails

* Start with ControlNet strength 0.85, guidance 7.5, and denoise around 0.42 to 0.45 for pool scenes. This reduces background drift and helps your ΔE00 edge metric pass.
* Keep pose alignment simple at first. Your `pose-alignment.ts` can evolve to snap pelvis to coping and toes to a waterline using `scene_analysis.json` as input.&#x20;
* Replace the Qwen and Nano stubs with real API calls when your keys and endpoints are ready. The scoring logic already handles selection.

If you want, I can also add a small Drizzle schema patch for persisting `projects`, `variants`, and `quality_metrics` using your existing `shared/schema.ts` and `server/storage.ts` layout so the metrics survive restarts.&#x20;
