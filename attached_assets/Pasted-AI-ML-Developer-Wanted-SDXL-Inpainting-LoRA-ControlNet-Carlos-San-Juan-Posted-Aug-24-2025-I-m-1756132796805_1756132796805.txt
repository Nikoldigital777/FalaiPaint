AI/ML Developer Wanted – SDXL Inpainting + LoRA + ControlNet
Carlos San Juan
Posted Aug 24, 2025
I’m a lifestyle photographer looking for an experienced AI/ML developer to help me build a custom image generation workflow. The goal is simple: keep the original photo background 100% intact, while seamlessly inserting photorealistic human figures that match a defined style.
What you’ll do:
• Train a foreground-only LoRA on SDXL Inpainting 0.1.
• Combine it with ControlNet (Pose + Depth) for realistic placement and scale.
• Build an inference pipeline that integrates LoRA + ControlNets + color management.
• Implement a clean mask generator from pose presets.
• Add a color pipeline: local Color Match (figure only) + global LUT for final look.
Deliverables:
• Foreground LoRA model.
• Training + inference workflow (scripts and ComfyUI).
• Mask/pose adaptation module.
• QA scripts (SSIM, pose accuracy, ΔE00 color check).
• Documentation + guide.
You are:
• Proven track record with SDXL (esp. Inpainting).
• Experienced with LoRA training (Kohya, diffusers, ComfyUI).
• Skilled with ControlNet (Pose/Depth).
• Solid CV background (segmentation, depth, pose skeletons).
• Strong Python (PyTorch, diffusers, OpenCV).
Trial project (paid, 1 week):
• Take one scene type (e.g. pool edge).
• Adapt pose preset + generate mask.
• Run SDXL Inpaint + LoRA.
• Apply color match + LUT.
• Deliver 3 variants + QA metrics.
Collaboration:
• Remote, async.
• Budget based on skills/experience.
• Opportunity for long-term work adding new scene types.
If you’ve built real pipelines with SDXL Inpaint + LoRA + ControlNet, let’s talk.
AI/ML Developer Wanted – SDXL Inpainting + LoRA + ControlNet Carlos San Juan Posted Aug 24, 2025 I’m a lifestyle photographer looking for an experienced AI/ML developer to help me build a custom image generation workflow. The goal is simple: keep the original photo background 100% intact, while seamlessly inserting photorealistic human figures that match a defined style. What you’ll do: • Train a foreground-only LoRA on SDXL Inpainting 0.1. • Combine it with ControlNet (Pose + Depth) for realistic placement and scale. • Build an inference pipeline that integrates LoRA + ControlNets + color management. • Implement a clean mask generator from pose presets. • Add a color pipeline: local Color Match (figure only) + global LUT for final look. Deliverables: • Foreground LoRA model. • Training + inference workflow (scripts and ComfyUI). • Mask/pose adaptation module. • QA scripts (SSIM, pose accuracy, ΔE00 color check). • Documentation + guide. You are: • Proven track record with SDXL (esp. Inpainting). • Experienced with LoRA training (Kohya, diffusers, ComfyUI). • Skilled with ControlNet (Pose/Depth). • Solid CV background (segmentation, depth, pose skeletons). • Strong Python (PyTorch, diffusers, OpenCV). Trial project (paid, 1 week): • Take one scene type (e.g. pool edge). • Adapt pose preset + generate mask. • Run SDXL Inpaint + LoRA. • Apply color match + LUT. • Deliver 3 variants + QA metrics. Collaboration: • Remote, async. • Budget based on skills/experience. • Opportunity for long-term work adding new scene types. If you’ve built real pipelines with SDXL Inpaint + LoRA + ControlNet, let’s talk.




How to use ControlNet with SDXL. Including perfect hands ...
Easy Inpainting for ANY model (SDXL, Flux, etc)
High-resolution advanced inpainting ComfyUI (rgthree, IP-Adapter ...
The job posting by Carlos San Juan is for an AI/ML developer to build a custom image generation workflow with the goal to keep the original photo background intact and insert photorealistic human figures matching a defined style. The core tasks include training a foreground-only LoRA on SDXL Inpainting 0.1, combining it with ControlNet (Pose + Depth) for realistic figure placement and scale, and building an inference pipeline integrating LoRA, ControlNet, and color management with clean mask generation from pose presets.
Key deliverables are:
A foreground LoRA model.
Training and inference workflow scripts alongside ComfyUI integration.
Mask/pose adaptation modules.
QA scripts to measure similarity, pose accuracy, and color differences.
Documentation and usage guide.
The role requires expertise with:
SDXL, especially Inpainting.
LoRA training (Kohya, diffusers, ComfyUI).
ControlNet usage with Pose and Depth conditioning.
Computer vision tasks like segmentation, depth estimation, and pose skeletons.
Strong Python skills (PyTorch, diffusers, OpenCV).
A paid one-week trial project involves adapting one scene (e.g., pool edge) to generate 3 variants with QA metrics, applying pose preset masks, SDXL Inpaint + LoRA, and color adjustments. Collaboration is remote and asynchronous, with budget based on skills and experience. Long-term work opportunities exist to expand to new scene types.
This workflow combines advanced AI techniques in image generation, control, and color grading for high-quality figure insertion while preserving backgrounds, leveraging state-of-the-art models and pipelines including SDXL Inpainting, LoRA, ControlNet, and custom masking/color matching integration.
The discussion also aligns with recent capabilities of ControlNet for SDXL inpainting and integration with compositional control using pose and depth preprocessing, as well as training custom LoRA models for style adaptation and figure insertions.
is this the right tech stack?
is this the right tech stack?

Yes, the described tech stack is the right and current state-of-the-art approach for the targeted custom image generation workflow:
SDXL Inpainting provides high-quality masked inpainting capability to keep original background intact while modifying specific regions.
LoRA training on the foreground figures allows fine-tuning and style adaptation efficiently without retraining the entire model.
ControlNet with Pose + Depth conditioning ensures realistic figure placement, scale, and pose control within the scene.
The pipeline including LoRA + ControlNet integration, color pipelines (local figure color matching + global LUT), and mask generation from pose presets aligns well with best practices seen in recent ComfyUI workflows for SDXL inpainting tasks.
Using QA metrics like SSIM, pose accuracy, and ΔE00 color difference is also standard for evaluating model output quality.
The use of Python with PyTorch, diffusers, OpenCV, and tools like Kohya and ComfyUI is standard for building and running these workflows.
This approach matches well with documented workflows and community-shared pipelines from 2024-2025 leveraging LoRA, ControlNet, and SDXL for inpainting and high-fidelity figure integration.
his budget is $800 - $1,200 • One-time
his budget is $800 - $1,200 • One-time




Carlos San Juan's budget of $800 - $1,200 (one-time) for this AI/ML development project appears modest given typical AI developer rates in 2025.
AI/ML developer hourly rates in the US generally range from around $42 to $80 per hour for mid-level developers, with averages near $53-$55 per hour. Senior AI engineers can command hourly rates between $80 to over $120+ per hour depending on experience and expertise. Freelance to small firm rates tend to span $50 to $300 per hour based on skill level and location.
Assuming a mid-level hourly rate of $50-$60, the budget corresponds roughly to 16 to 24 hours of work. For a complex workflow involving SDXL Inpainting, LoRA training, ControlNet integration, custom pipeline development, QA metrics, and documentation—as described—it is quite tight for one week of professional work in North America.
If hiring remotely from lower-cost regions or junior to mid-level developers, the budget may be feasible with clear scope control or a minimal trial phase. For highly experienced specialists or larger scope, expectations around budget or timeline would need adjustment.
In summary, the budget is on the low end for the described scope and expertise level in the US market but might work under certain arrangements with careful scope management or remote talent sourcing.
Owen image edit?
Owen image edit?


"Owen image edit" likely refers to Qwen-Image-Edit, a cutting-edge AI photo editing model released by Alibaba in 2025. It enables editing images through natural language commands, such as changing colors, adding or replacing objects and people, swapping backgrounds, altering textures, and changing facial expressions or clothing styles. Qwen-Image-Edit is known for precise and intelligent editing that supports complex changes in a user-friendly way, often through web interfaces.
This model is built on a large 20-billion parameter base and offers advanced capabilities in segmentation and text-rich image editing that make it suitable for a wide range of creative and realistic photo edits. It complements other tools like SDXL Inpainting and ControlNet workflows by offering a strong image editing solution that can work with human-readable instructions.
AI/ML Developer Wanted – SDXL Inpainting + LoRA + ControlNet Carlos San Juan Posted Aug 24, 2025 I’m a lifestyle photographer looking for an experienced AI/ML developer to help me build a custom image generation workflow. The goal is simple: keep the original photo background 100% intact, while seamlessly inserting photorealistic human figures that match a defined style. What you’ll do: • Train a foreground-only LoRA on SDXL Inpainting 0.1. • Combine it with ControlNet (Pose + Depth) for realistic placement and scale. • Build an inference pipeline that integrates LoRA + ControlNets + color management. • Implement a clean mask generator from pose presets. • Add a color pipeline: local Color Match (figure only) + global LUT for final look. Deliverables: • Foreground LoRA model. • Training + inference workflow (scripts and ComfyUI). • Mask/pose adaptation module. • QA scripts (SSIM, pose accuracy, ΔE00 color check). • Documentation + guide. You are: • Proven track record with SDXL (esp. Inpainting). • Experienced with LoRA training (Kohya, diffusers, ComfyUI). • Skilled with ControlNet (Pose/Depth). • Solid CV background (segmentation, depth, pose skeletons). • Strong Python (PyTorch, diffusers, OpenCV). Trial project (paid, 1 week): • Take one scene type (e.g. pool edge). • Adapt pose preset + generate mask. • Run SDXL Inpaint + LoRA. • Apply color match + LUT. • Deliver 3 variants + QA metrics. Collaboration: • Remote, async. • Budget based on skills/experience. • Opportunity for long-term work adding new scene types. If you’ve built real pipelines with SDXL Inpaint + LoRA + ControlNet, let’s talk. AI/ML Developer Wanted – SDXL Inpainting + LoRA + ControlNet Carlos San Juan Posted Aug 24, 2025 I’m a lifestyle photographer looking for an experienced AI/ML developer to help me build a custom image generation workflow. The goal is simple: keep the original photo background 100% intact, while seamlessly inserting photorealistic human figures that match a defined style. What you’ll do: • Train a foreground-only LoRA on SDXL Inpainting 0.1. • Combine it with ControlNet (Pose + Depth) for realistic placement and scale. • Build an inference pipeline that integrates LoRA + ControlNets + color management. • Implement a clean mask generator from pose presets. • Add a color pipeline: local Color Match (figure only) + global LUT for final look. Deliverables: • Foreground LoRA model. • Training + inference workflow (scripts and ComfyUI). • Mask/pose adaptation module. • QA scripts (SSIM, pose accuracy, ΔE00 color check). • Documentation + guide. You are: • Proven track record with SDXL (esp. Inpainting). • Experienced with LoRA training (Kohya, diffusers, ComfyUI). • Skilled with ControlNet (Pose/Depth). • Solid CV background (segmentation, depth, pose skeletons). • Strong Python (PyTorch, diffusers, OpenCV). Trial project (paid, 1 week): • Take one scene type (e.g. pool edge). • Adapt pose preset + generate mask. • Run SDXL Inpaint + LoRA. • Apply color match + LUT. • Deliver 3 variants + QA metrics. Collaboration: • Remote, async. • Budget based on skills/experience. • Opportunity for long-term work adding new scene types. If you’ve built real pipelines with SDXL Inpaint + LoRA + ControlNet, let’s talk. How to use ControlNet with SDXL. Including perfect hands ... Easy Inpainting for ANY model (SDXL, Flux, etc) High-resolution advanced inpainting ComfyUI (rgthree, IP-Adapter ... The job posting by Carlos San Juan is for an AI/ML developer to build a custom image generation workflow with the goal to keep the original photo background intact and insert photorealistic human figures matching a defined style. The core tasks include training a foreground-only LoRA on SDXL Inpainting 0.1, combining it with ControlNet (Pose + Depth) for realistic figure placement and scale, and building an inference pipeline integrating LoRA, ControlNet, and color management with clean mask generation from pose presets. Key deliverables are: A foreground LoRA model. Training and inference workflow scripts alongside ComfyUI integration. Mask/pose adaptation modules. QA scripts to measure similarity, pose accuracy, and color differences. Documentation and usage guide. The role requires expertise with: SDXL, especially Inpainting. LoRA training (Kohya, diffusers, ComfyUI). ControlNet usage with Pose and Depth conditioning. Computer vision tasks like segmentation, depth estimation, and pose skeletons. Strong Python skills (PyTorch, diffusers, OpenCV). A paid one-week trial project involves adapting one scene (e.g., pool edge) to generate 3 variants with QA metrics, applying pose preset masks, SDXL Inpaint + LoRA, and color adjustments. Collaboration is remote and asynchronous, with budget based on skills and experience. Long-term work opportunities exist to expand to new scene types. This workflow combines advanced AI techniques in image generation, control, and color grading for high-quality figure insertion while preserving backgrounds, leveraging state-of-the-art models and pipelines including SDXL Inpainting, LoRA, ControlNet, and custom masking/color matching integration. The discussion also aligns with recent capabilities of ControlNet for SDXL inpainting and integration with compositional control using pose and depth preprocessing, as well as training custom LoRA models for style adaptation and figure insertions. is this the right tech stack? is this the right tech stack? Yes, the described tech stack is the right and current state-of-the-art approach for the targeted custom image generation workflow: SDXL Inpainting provides high-quality masked inpainting capability to keep original background intact while modifying specific regions. LoRA training on the foreground figures allows fine-tuning and style adaptation efficiently without retraining the entire model. ControlNet with Pose + Depth conditioning ensures realistic figure placement, scale, and pose control within the scene. The pipeline including LoRA + ControlNet integration, color pipelines (local figure color matching + global LUT), and mask generation from pose presets aligns well with best practices seen in recent ComfyUI workflows for SDXL inpainting tasks. Using QA metrics like SSIM, pose accuracy, and ΔE00 color difference is also standard for evaluating model output quality. The use of Python with PyTorch, diffusers, OpenCV, and tools like Kohya and ComfyUI is standard for building and running these workflows. This approach matches well with documented workflows and community-shared pipelines from 2024-2025 leveraging LoRA, ControlNet, and SDXL for inpainting and high-fidelity figure integration. his budget is $800 - $1,200 • One-time his budget is $800 - $1,200 • One-time Carlos San Juan's budget of $800 - $1,200 (one-time) for this AI/ML development project appears modest given typical AI developer rates in 2025. AI/ML developer hourly rates in the US generally range from around $42 to $80 per hour for mid-level developers, with averages near $53-$55 per hour. Senior AI engineers can command hourly rates between $80 to over $120+ per hour depending on experience and expertise. Freelance to small firm rates tend to span $50 to $300 per hour based on skill level and location. Assuming a mid-level hourly rate of $50-$60, the budget corresponds roughly to 16 to 24 hours of work. For a complex workflow involving SDXL Inpainting, LoRA training, ControlNet integration, custom pipeline development, QA metrics, and documentation—as described—it is quite tight for one week of professional work in North America. If hiring remotely from lower-cost regions or junior to mid-level developers, the budget may be feasible with clear scope control or a minimal trial phase. For highly experienced specialists or larger scope, expectations around budget or timeline would need adjustment. In summary, the budget is on the low end for the described scope and expertise level in the US market but might work under certain arrangements with careful scope management or remote talent sourcing. Owen image edit? Owen image edit? "Owen image edit" likely refers to Qwen-Image-Edit, a cutting-edge AI photo editing model released by Alibaba in 2025. It enables editing images through natural language commands, such as changing colors, adding or replacing objects and people, swapping backgrounds, altering textures, and changing facial expressions or clothing styles. Qwen-Image-Edit is known for precise and intelligent editing that supports complex changes in a user-friendly way, often through web interfaces. This model is built on a large 20-billion parameter base and offers advanced capabilities in segmentation and text-rich image editing that make it suitable for a wide range of creative and realistic photo edits. It complements other tools like SDXL Inpainting and ControlNet workflows by offering a strong image editing solution that can work with human-readable instructions.
The described tech stack for the AI/ML developer role involving SDXL Inpainting + LoRA + ControlNet is indeed the right and current state-of-the-art approach for building a custom image generation workflow with goals like preserving the photo background while inserting photorealistic human figures.

Key points confirming this include:

SDXL Inpainting 0.1 enables high-quality masked inpainting, ideal for keeping backgrounds intact while modifying foreground areas.

LoRA training focused on foreground figures provides efficient fine-tuning for stylistic adaptation without retraining the full model.

ControlNet conditioning using Pose and Depth is crucial for realistic figure placement, pose, and scale control in the scene.

Integration of LoRA and ControlNet into an inference pipeline with clean mask generation and color matching pipelines (local figure color match plus global LUT) aligns with best practices seen in recent workflows.

QA metrics like SSIM for structural similarity, pose accuracy, and ΔE00 color difference are standard for validating output quality.

The use of Python with libraries like PyTorch, diffusers, and OpenCV, plus UI integration with ComfyUI and training tools like Kohya, matches typical development environments for these tasks.

Recent references (2024-2025) show this combined approach is actively used and recommended in professional and community workflows for SDXL-based inpainting and figure integration.

In sum, this is a well-chosen tech stack for the project goals described by Carlos San Juan, combining advanced AI models, fine-tuning strategies, compositional control, and color management into a clean, high-quality image generation pipeline.

Regarding the budget of $800 - $1,200 for this scope, it is on the modest side in the US market for the expertise and work involved but could work with remote or junior-mid level developers with careful scope management.

If there are any particular doubts or clarifications needed about parts of the tech stack or workflow, I can provide further detail.

how to build?
how to build?
Comfyui Tutorial: LoRA Inpainting Process with Turbo model ... trailer
Comfyui Tutorial: LoRA Inpainting Process with Turbo model ...
Flux Workflows: Updated Models, ControlNet & LoRa in ComfyUI trailer
Flux Workflows: Updated Models, ControlNet & LoRa in ComfyUI
ComfyUI Masterclass Part 8: How to Use LoRAs (SDXL & FLUX) trailer
ComfyUI Masterclass Part 8: How to Use LoRAs (SDXL & FLUX)

Building a custom image generation workflow using SDXL Inpainting + LoRA + ControlNet involves several key steps, focused on training, integration, and pipeline construction. Here's a high-level guide to build it:

Prepare Data and Define Scope

Collect training data with foreground human figures and original photo backgrounds.

Define mask regions and pose presets for the human figures to train LoRA on.

Train Foreground LoRA on SDXL Inpainting

Use an SDXL Inpainting base model version 0.1.

Train LoRA only on the figure foreground areas, preserving background.

Tools like Kohya, diffusers, or ComfyUI can facilitate this LoRA training.

Configure training with appropriate masks and pose-conditioned data.

Set Up ControlNet for Pose + Depth Conditioning

Use ControlNet models trained for human pose and depth maps.

These guide figure placement, scale, and realistic pose integration in images.

Prepare pose skeletons and depth maps for conditioning inputs.

Build Inference Pipeline

Integrate the SDXL Inpainting base with the trained LoRA foreground model.

Add ControlNet modules for pose and depth.

Implement mask generation from pose presets for clean figure region selection.

Add a color pipeline: local color matching for figures plus global LUT for unified look.

Quality Assurance (QA) and Metrics

Develop scripts for SSIM (structural similarity) to check output fidelity.

Pose accuracy measures to verify realistic figure placement.

Color difference metrics like ΔE00 to ensure color consistency.

Documentation and User Guide

Document training and inference workflows, including scripts in Python.

Provide usage guides for running the pipeline, mask generation, and QA.

Tools and Libraries:

Python ecosystem: PyTorch, diffusers, OpenCV.

UI & workflow: ComfyUI.

Training: Kohya LoRA trainer or diffusers training scripts.

Dataset preparation: pose extraction & mask creation with computer vision.

With these components, the goal is to keep the photo background 100% intact while inserting photorealistic human figures that match a defined style through LoRA, ControlNet control, and color matching.

If needed, more detailed coding or training examples can be provided.
ChatGPT said:
Got it — you want a clear, step-by-step build guide for this SDXL Inpainting + LoRA + ControlNet pipeline, using ComfyUI as the workflow manager and Python for automation. I’ll break it down into seven main phases so you can start small, test quickly, and scale up.
1. Set Up Your Environment
Dependencies
# Create a clean virtual environment
python3 -m venv sdxl_env
source sdxl_env/bin/activate

# Core libraries
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install diffusers transformers accelerate
pip install opencv-python numpy pillow
pip install xformers peft
pip install controlnet_aux
pip install matplotlib seaborn tqdm
Optional but recommended:
ComfyUI → Workflow builder for SDXL + LoRA + ControlNet.
Kohya LoRA Trainer → Best for efficient LoRA training.
Qwen-Image-Edit (optional) → For fine-grained human figure adjustments.
2. Prepare Your Dataset
a. Collect Foreground Figures
500–3,000 high-resolution human figures.
Diverse poses, lighting, and clothing styles.
Transparent backgrounds preferred.
b. Extract Poses + Depth Maps
We'll preprocess with ControlNet’s auxiliary tools:
from controlnet_aux import OpenposeDetector, MidasDetector
from PIL import Image

pose_detector = OpenposeDetector.from_pretrained("lllyasviel/ControlNet")
depth_detector = MidasDetector.from_pretrained("lllyasviel/midas")

image = Image.open("sample_person.jpg")
pose = pose_detector(image)
depth = depth_detector(image)
pose.save("pose_map.png")
depth.save("depth_map.png")
3. Train the Foreground-Only LoRA
Why LoRA Here
We’re not fine-tuning SDXL itself — we’re teaching the model to stylize foreground humans only.
Training Command (Kohya):

accelerate launch kohya/train_network.py \
  --pretrained_model_name_or_path="stabilityai/stable-diffusion-xl-base-1.0" \
  --train_data_dir="./dataset/foreground" \
  --resolution=1024,1024 \
  --output_dir="./lora_model" \
  --network_module=networks.lora \
  --learning_rate=1e-4 \
  --train_batch_size=4 \
  --max_train_steps=8000 \
  --mixed_precision="fp16"
Tips:
Always train on masked data — keep backgrounds transparent.
Use captions that describe style, clothing, and ethnicity if needed.
4. Build the Inference Pipeline
We'll merge SDXL Inpainting + LoRA + ControlNet:
from diffusers import StableDiffusionXLInpaintPipeline, ControlNetModel
from PIL import Image

# Load LoRA + ControlNets
pose_cn = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-openpose")
depth_cn = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-depth")

pipe = StableDiffusionXLInpaintPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    controlnet=[pose_cn, depth_cn],
    torch_dtype=torch.float16
)

pipe.load_lora_weights("lora_model", adapter_name="foreground_lora")
pipe.to("cuda")

# Inputs
background = Image.open("scene.jpg")
mask = Image.open("mask.png")
pose = Image.open("pose_map.png")
depth = Image.open("depth_map.png")

result = pipe(
    prompt="A stylish woman wearing a sundress, cinematic lighting",
    image=background,
    mask_image=mask,
    controlnet_conditioning_images=[pose, depth],
    num_inference_steps=30,
).images[0]

result.save("output.jpg")
5. Add Color Matching + LUT Integration
a. Local Color Match (Figure Only)
import cv2
import numpy as np

def color_transfer(src_path, target_path, mask_path, output_path):
    src = cv2.imread(src_path)
    tgt = cv2.imread(target_path)
    mask = cv2.imread(mask_path, 0)

    src_mean, src_std = cv2.meanStdDev(src, mask=mask)
    tgt_mean, tgt_std = cv2.meanStdDev(tgt, mask=mask)

    normalized = ((src - src_mean) * (tgt_std / src_std)) + tgt_mean
    normalized = np.clip(normalized, 0, 255).astype(np.uint8)
    cv2.imwrite(output_path, normalized)
b. Apply a Global LUT for Cinematic Grading
lut = cv2.imread("cinematic_lut.cube", cv2.IMREAD_UNCHANGED)
graded = cv2.LUT(result, lut)
cv2.imwrite("final_output.jpg", graded)
6. Add QA Metrics
a. Structural Similarity (SSIM)
from skimage.metrics import structural_similarity as ssim
import cv2

ref = cv2.imread("original.jpg")
out = cv2.imread("final_output.jpg")
ssim_score = ssim(ref, out, channel_axis=2)
print("SSIM:", ssim_score)
b. ΔE00 Color Consistency
from skimage import color
lab1 = color.rgb2lab(ref)
lab2 = color.rgb2lab(out)
delta_e = np.mean(color.deltaE_ciede2000(lab1, lab2))
print("ΔE00:", delta_e)
7. ComfyUI Workflow Setup
Inside ComfyUI:
Node 1: SDXL Inpainting Base.
Node 2: Load LoRA (Foreground).
Node 3: ControlNet Pose.
Node 4: ControlNet Depth.
Node 5: Mask Generator.
Node 6: Color Matching LUT Node.
Node 7: Preview & Save.
This lets you drag-and-drop scene presets, select masks, and get instant visual feedback.
Next Steps
If you want, I can:
Create a full ComfyUI JSON workflow with these nodes pre-connected.
Provide a dataset structure guide for LoRA training.
Build an automated training + inference CLI to streamline the entire process.
Do you want me to start by making you a ComfyUI-ready workflow JSON so you can test SDXL Inpainting + LoRA + ControlNet immediately? It’ll save you days of setup. Should I?